# ddoc v2.0.2 릴리스 노트

## 릴리스 일자
2025-11-25

## 🎯 워크스페이스 캐시 동기화 및 증분 분석 안정화

ddoc v2.0.2는 데이터 해시 기반 캐시 관리 시스템과 스냅샷 기반 분석 기능을 도입한 주요 업데이트입니다. 특히 워크스페이스 캐시 자동 동기화와 증분 분석 안정화를 통해 데이터 변경 시에도 캐시가 정확하게 유지되며, 분석 결과의 효율적인 저장 및 재사용, 데이터 해시 일관성 보장을 통해 성능과 안정성을 크게 향상시켰습니다.

## ✨ New Features

### 1. Workspace Cache Synchronization (워크스페이스 캐시 자동 동기화) 🆕

#### Workspace State Tracking
- **`workspace_state.json`**: 현재 워크스페이스의 data_hash 추적
- 데이터 변경 시 이전 data_hash와 현재 data_hash 자동 비교
- 캐시와 실제 데이터 상태의 일관성 보장

#### Automatic Cache Migration
- **`sync_workspace_cache()`** 메서드: data_hash 변경 시 자동 캐시 복사
- EDA 실행 전 자동 캐시 동기화
- Snapshot checkout 시 자동 캐시 동기화
- 증분 분석이 항상 올바른 캐시를 참조하도록 보장

#### Unified Cache Loading
- workspace는 `workspace_state.json`의 current_data_hash 사용
- snapshot은 snapshot YAML의 data_hash 사용
- `load_analysis_cache()`와 `load_file_metadata()` 통일된 로직

**워크플로우 예시**:
```bash
# 1. v01 생성 (data_hash: abc123)
ddoc analyze eda --save-snapshot

# 2. 데이터 변경 (500개 파일 삭제)
rm -rf data/valid/

# 3. EDA 실행 (data_hash: def456)
ddoc analyze eda
# → 자동 감지: abc123 → def456
# → 캐시 복사 후 증분 분석
# → removed: 500개 정확히 감지

# 4. v02 스냅샷 저장
ddoc snapshot -m "removed valid" -a v02

# 5. Drift 분석
ddoc analyze drift v01 v02
# → 정상 작동! ✅
```

### 2. Data Hash-based Cache Management (데이터 해시 기반 캐시 관리)

#### 하이브리드 캐시 구조
- **파일 시스템 + SQLite 인덱싱**
  - 대용량 바이너리 데이터(embeddings)는 파일 시스템에 저장
  - 메타데이터와 인덱스는 SQLite에 저장
  - `.ddoc/cache/data/{data_hash}/` 디렉토리 구조

#### SQLite 인덱스 데이터베이스 (`index.db`)
- `snapshot_mapping` 테이블: 스냅샷 ID → 데이터 해시 매핑
- `data_cache` 테이블: 데이터 해시별 캐시 메타데이터
- 효율적인 조회 및 중복 제거
- 동일 데이터 해시를 가진 여러 스냅샷이 캐시 공유

#### 캐시 복사 기능
- `copy_cache()` 메서드로 데이터 해시 변경 시 기존 캐시를 새 해시로 복사
- 스냅샷 생성 중 자동 캐시 마이그레이션
- 분석 결과 손실 방지

### 2. Snapshot-based Analysis (스냅샷 기반 분석)

#### 통합 분석 시스템
- **워크스페이스와 스냅샷 모두 지원**
  - `ddoc analyze eda`: 현재 워크스페이스 상태 분석
  - `ddoc analyze eda <snapshot_id>`: 특정 스냅샷 분석
  - `--save-snapshot` 옵션: 분석 후 자동 스냅샷 생성

#### 증분 분석 지원
- 파일 단위 메타데이터 추적 (`FileMetadata` 스키마)
- 변경된 파일만 재분석하여 성능 최적화
- 상대 경로 기반 파일 키로 다중 데이터셋 지원
- 중첩 디렉토리 구조 지원 (`rglob` 사용)

#### Drift Detection 개선
- 스냅샷 간 비교 분석
- 각 스냅샷의 데이터 해시 기반 캐시 자동 로드
- 정확한 버전 간 차이 분석

### 3. Data Hash Consistency (데이터 해시 일관성 보장)

#### 자동 DVC 추적 확인
- 분석 전 `dvc add data/` 자동 실행
- `_ensure_data_tracked()` 메서드로 데이터 해시 안정화
- 분석과 스냅샷 생성 간 해시 불일치 방지

#### 시스템 파일 무시
- `.dvcignore` 자동 생성
- `.DS_Store`, `Thumbs.db` 등 시스템 파일 제외
- DVC 해시 안정성 향상

#### 해시 변경 감지
- 분석 중 데이터 변경 모니터링
- 분석 전후 해시 비교
- 변경 시 경고 메시지 출력

### 4. Enhanced Snapshot Management (스냅샷 관리 개선)

#### Alias 중복 방지
- 동일 alias 재사용 시 에러 반환
- 명확한 에러 메시지 제공
- 혼동 방지를 위한 엄격한 검증

#### 데이터 변경 감지 개선
- `_has_data_changes()` 메서드 개선
- `dvc status` 출력 정확한 파싱
- "Pipeline is up to date" 메시지 무시
- 실제 변경사항만 감지

## 🔄 Changed

### Cache Service Architecture (캐시 서비스 아키텍처 변경)

#### 파일 기반 → 하이브리드 구조
- **기존**: 파일 시스템만 사용
- **변경**: 파일 시스템 + SQLite 인덱스

#### 캐시 저장 위치
- **기존**: `.ddoc/cache/{snapshot_id}/`
- **변경**: `.ddoc/cache/data/{data_hash}/`

#### 스냅샷 매핑
- **기존**: JSON 파일 기반 매핑
- **변경**: SQLite 테이블 기반 매핑

### Analysis Hook Signatures (분석 Hook 시그니처 변경)

#### `eda_run()` 시그니처 업데이트
```python
def eda_run(
    snapshot_id: str,
    data_path: str,
    data_hash: str,
    output_path: str,
    invalidate_cache: bool = False
) -> Optional[Dict[str, Any]]
```

#### `drift_detect()` 시그니처 업데이트
```python
def drift_detect(
    snapshot_id_ref: str,
    snapshot_id_cur: str,
    data_path_ref: str,
    data_path_cur: str,
    data_hash_ref: str,
    data_hash_cur: str,
    detector: str,
    cfg: Dict[str, Any],
    output_path: str
) -> Optional[Dict[str, Any]]
```

### Plugin Integration (플러그인 통합)

#### Vision Plugin 업데이트
- 새로운 hook 시그니처에 맞춘 구현
- `CacheService` 직접 사용 (자체 캐시 매니저 제거)
- 상대 경로 기반 파일 키로 다중 데이터셋 지원
- `summary` 캐시 저장 추가 (drift 분석용)

## 📦 New Core Modules & Methods

### CacheService 확장
- `_init_index()`: SQLite 인덱스 초기화
- `_save_snapshot_mapping()`: 스냅샷-데이터 해시 매핑 저장
- `_get_data_hash_by_snapshot()`: 스냅샷 ID로 데이터 해시 조회
- `find_snapshots_by_data_hash()`: 동일 데이터 해시를 가진 스냅샷 조회
- `copy_cache()`: 캐시 복사 (해시 변경 시)
- `compute_incremental_changes()`: 증분 분석용 변경사항 계산
- `save_file_metadata()` / `load_file_metadata()`: 파일별 메타데이터 관리
- **🆕 `_get_workspace_state()`**: 현재 워크스페이스 data_hash 조회
- **🆕 `_update_workspace_state()`**: 워크스페이스 data_hash 업데이트
- **🆕 `sync_workspace_cache()`**: data_hash 변경 시 자동 캐시 동기화

### SnapshotService 확장
- `get_workspace_state()`: 현재 워크스페이스 상태 반환
- `get_or_create_workspace_snapshot()`: 워크스페이스 스냅샷 참조 생성
- `_ensure_data_tracked()`: DVC 추적 확인 및 업데이트
- `_has_data_changes()`: 데이터 변경 감지 개선
- `_set_alias()`: Alias 설정 시 엄격한 검증

### Schemas 확장
- `FileMetadata`: 파일 단위 메타데이터 스키마
  - `file_path`, `file_hash`, `file_size`, `file_mtime`, `analyzed_at`
- `AnalysisCache`: 증분 분석 정보 포함 확장
  - `file_metadata`: 파일별 메타데이터 딕셔너리
  - `incremental_info`: 증분 분석 정보

## 🐛 Bug Fixes

### 1. 증분 분석 시 빈 캐시 생성 문제 🆕 (Critical)
- **문제**: 데이터 변경 후 EDA 실행 시 빈 attributes.json 파일 생성
- **근본 원인**: 
  - `load_analysis_cache()`는 새로운 data_hash로 로드 시도 → 실패
  - `load_file_metadata()`는 snapshot_id로 이전 data_hash 자동 해결 → 성공
  - 결과: file_metadata는 있지만 attr_cache는 비어있음
  - `compute_incremental_changes()`는 removed 파일 감지하지만 처리할 캐시 없음
- **해결**: 
  - `workspace_state.json`을 통한 통일된 data_hash 추적
  - `sync_workspace_cache()`로 data_hash 변경 시 자동 캐시 복사
  - workspace와 snapshot 모두 일관된 캐시 로드 메커니즘

**재현 시나리오 (수정 전)**:
```bash
v01 생성 → valid 폴더 삭제 → EDA 실행 → v02 저장
                                  ↓
                         빈 캐시 생성! (attributes.json = {})
                                  ↓
                         drift 분석 실패 ❌
```

**수정 후**:
```bash
v01 생성 → valid 폴더 삭제 → EDA 실행 → v02 저장
                                  ↓
                         캐시 자동 복사 → 증분 분석
                                  ↓
                         drift 분석 성공 ✅
```

### 2. Drift 분석에서 잘못된 캐시 타입 로드 🆕
- **문제**: `drift.py`에서 `cache_type="summary"` 로드 → `vision_impl.py`는 attributes 데이터 기대
- **에러**: `TypeError: argument of type 'int' is not iterable`
  ```python
  # summary.json: {"num_files": 3894, "avg_size": 0.038, ...}
  # vision_impl.py: baseline_attr[f]['size'] ← f는 파일명이어야 하는데 int!
  ```
- **해결**: `drift.py`에서 `cache_type="attributes"` 로드하도록 수정

### 3. Drift.py 들여쓰기 오류 🆕
- **문제**: `IndentationError: unexpected indent` 발생
- **원인**: 49번 라인과 97번 라인의 잘못된 들여쓰기
- **해결**: 들여쓰기 수정

### 4. 캐시 매핑 문제 해결
- **문제**: `ddoc analyze eda`와 `ddoc snapshot` 간 데이터 해시 불일치
- **원인**: `.DS_Store` 등 시스템 파일로 인한 해시 변경
- **해결**: `.dvcignore`에 시스템 파일 추가, 분석 전 `dvc add` 강제 실행

### 5. Pluggy Hook 반환값 처리
- **문제**: Hook이 리스트를 반환할 때 `AttributeError` 발생
- **해결**: 리스트/딕셔너리 타입 확인 및 적절한 처리

### 6. 데이터 변경 감지 오류
- **문제**: 변경 없어도 "Data changes detected: True" 표시
- **해결**: `dvc status` 출력 정확한 파싱

### 7. Alias 중복 생성
- **문제**: 동일 alias가 여러 스냅샷에 할당 가능
- **해결**: 엄격한 검증 및 명확한 에러 메시지

## 🔧 Technical Improvements

### 1. 캐시 효율성 향상
- 동일 데이터 해시를 가진 여러 스냅샷이 캐시 공유
- SQLite 인덱스로 빠른 조회
- 증분 분석으로 불필요한 재계산 방지

### 2. 데이터 해시 안정성
- 시스템 파일 무시로 해시 안정성 향상
- 분석 전 자동 DVC 추적 확인
- 해시 변경 시 자동 캐시 마이그레이션

### 3. 다중 데이터셋 지원
- 상대 경로 기반 파일 키
- 중첩 디렉토리 구조 지원 (`rglob` 사용)
- 데이터셋별 독립적인 메타데이터 관리

## 🎯 Implementation Details

### 캐시 디렉토리 구조
```
.ddoc/cache/
├── data/
│   ├── {data_hash_1}/
│   │   ├── attributes.json
│   │   ├── embeddings.pkl
│   │   ├── file_metadata.json
│   │   └── summary.json
│   └── {data_hash_2}/
│       └── ...
├── index.db           # SQLite 인덱스
└── workspace_state.json  # 🆕 워크스페이스 상태 추적
```

### Workspace State 구조 🆕
```json
{
  "current_data_hash": "abc123...",
  "updated_at": "2025-11-25T14:30:00"
}
```

### SQLite 스키마
```sql
-- 스냅샷 ID → 데이터 해시 매핑
CREATE TABLE snapshot_mapping (
    snapshot_id TEXT PRIMARY KEY,
    data_hash TEXT NOT NULL,
    created_at TEXT NOT NULL
);

-- 데이터 해시별 캐시 메타데이터
CREATE TABLE data_cache (
    data_hash TEXT PRIMARY KEY,
    cache_types TEXT,  -- JSON array
    created_at TEXT NOT NULL,
    last_accessed TEXT
);
```

### 워크스페이스 캐시 동기화 플로우 🆕
```
1. 데이터 변경 (예: 500개 파일 삭제)
   └─> data_hash 변경: abc123 → def456

2. ddoc analyze eda 실행
   ├─> 현재 data_hash 조회: def456
   ├─> workspace_state.json 조회: abc123
   ├─> 불일치 감지!
   ├─> sync_workspace_cache(def456, abc123) 호출
   │   ├─> abc123 캐시 존재 확인
   │   ├─> abc123 → def456 캐시 복사
   │   └─> workspace_state.json 업데이트
   ├─> 복사된 캐시로부터 증분 분석
   │   ├─> removed: 500개 감지
   │   ├─> attr_cache에서 500개 제거
   │   └─> 갱신된 캐시 저장
   └─> 분석 완료 ✅

3. ddoc snapshot 실행
   └─> v02 → def456 매핑 저장

4. ddoc analyze drift v01 v02
   ├─> v01: abc123 캐시 로드 (3894 files)
   ├─> v02: def456 캐시 로드 (3392 files)
   └─> Drift 분석 성공 ✅
```

### 데이터 해시 일관성 보장 플로우
1. `ddoc analyze eda` 실행
2. `_ensure_data_tracked()` 호출 → `dvc add data/` 실행 (필요 시)
3. 현재 `data_hash` 조회
4. **🆕 `sync_workspace_cache()` 호출** → 캐시 자동 동기화
5. 분석 수행 및 캐시 저장 (`data_hash` 기반)
6. `ddoc snapshot` 실행
7. `_auto_commit_workflow()` → `dvc add data/` 실행
8. 해시 변경 감지 시 `copy_cache()` 호출
9. 스냅샷-해시 매핑 저장
10. **🆕 Checkout 시 `sync_workspace_cache()` 자동 호출**

## 📚 Updated Documentation

- 캐시 구조 및 해시 관리 방식 문서화
- 스냅샷 기반 분석 워크플로우 가이드
- 증분 분석 사용법

## 📝 마이그레이션 노트

### 호환성 유지
- ✅ 모든 v2.0.1 명령이 동일하게 작동
- ✅ 기존 캐시는 자동으로 새 구조로 마이그레이션
- ✅ 설정 변경 불필요

### 개발자를 위해
- 새로운 캐시 구조로 더 효율적인 분석 가능
- 증분 분석으로 대용량 데이터셋 처리 성능 향상
- SQLite 인덱스로 빠른 캐시 조회

## 🙏 감사의 말

이번 릴리스는 데이터 관리와 분석 효율성에 중점을 두어, ddoc을 대규모 데이터셋을 다루는 사용자에게 더욱 강력한 도구로 만듭니다.

---

**전체 변경 이력**: v2.0.1과 v2.0.2 사이의 git log 참조

