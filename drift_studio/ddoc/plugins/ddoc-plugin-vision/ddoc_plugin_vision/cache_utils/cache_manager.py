import os
import json
import hashlib
import pickle
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Iterable, Optional, Tuple

from .cache_repository import CacheRepository

'''
dataset_folder/
â”œâ”€â”€ cache/
â”‚   â”œâ”€â”€ analysis_image_analysis_test_data2.cache
â”‚   â”œâ”€â”€ analysis_image_analysis_test_data2_meta.json
â”‚   â”œâ”€â”€ analysis_image_drift_content_test_data2.cache
â”‚   â”œâ”€â”€ analysis_image_drift_content_test_data2_meta.json
â”‚   â”œâ”€â”€ analysis_xai_analysis_test_data2.cache
â”‚   â””â”€â”€ analysis_xai_analysis_test_data2_meta.json
'''
DEFAULT_CACHE_DATA_TYPES: Tuple[str, ...] = (
    "attribute_analysis",
    "embedding_analysis",
)


class CacheManager:
    def __init__(self, dataset_directory):
        """ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” (ë°ì´í„°ì…‹ë³„ ìºì‹œë§Œ ì§€ì›)"""
        if not dataset_directory:
            raise ValueError("dataset_directoryëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. ì „ì—­ ìºì‹œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        # ë°ì´í„°ì…‹ë³„ ìºì‹œ ë””ë ‰í† ë¦¬
        self.cache_dir = Path(dataset_directory) / 'cache'
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        
        # ìºì‹œ ì„¤ì •
        self.cache_expiry_days = int(os.getenv('DATADRIFT_CACHE_EXPIRY_DAYS', '30'))
        self.max_cache_size_mb = int(os.getenv('DATADRIFT_CACHE_MAX_SIZE_MB', '999999'))
    
    def _get_cache_key(self, identifier, content_type="html"):
        """ìºì‹œ í‚¤ ìƒì„±"""
        # ì‹ë³„ìë¥¼ ì•ˆì „í•œ íŒŒì¼ëª…ìœ¼ë¡œ ë³€í™˜
        safe_identifier = self._make_safe_filename(identifier)
        
        # íŒŒì¼ëª… ê¸¸ì´ ì œí•œ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡)
        if len(safe_identifier) > 50:
            # í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ ì§§ê²Œ ë§Œë“¤ê¸°
            hash_obj = hashlib.md5(identifier.encode())
            safe_identifier = f"{safe_identifier[:30]}_{hash_obj.hexdigest()[:8]}"

        prefix = f"{content_type}_" if content_type else ""
        return f"{prefix}{safe_identifier}"
    
    def _make_safe_filename(self, filename):
        """íŒŒì¼ëª…ì„ ì•ˆì „í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤."""
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° ë° ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€ê²½
        import re
        safe_name = re.sub(r'[^\w\-_.]', '_', filename)
        # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ ë³€ê²½
        safe_name = re.sub(r'_+', '_', safe_name)
        # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
        safe_name = safe_name.strip('_')
        return safe_name
    
    def _get_cache_path(self, cache_key):
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return self.cache_dir / f"{cache_key}.cache"
    
    def _get_metadata_path(self, cache_key):
        """ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return self.cache_dir / f"{cache_key}_meta.json"
    
    def _is_cache_valid(self, metadata_path):
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸"""
        if not metadata_path.exists():
            return False
        
        try:
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # ë§Œë£Œ ì‹œê°„ í™•ì¸
            created_time = datetime.fromisoformat(metadata['created_time'])
            expiry_time = created_time + timedelta(days=self.cache_expiry_days)
            
            return datetime.now() < expiry_time
        except Exception:
            return False
    
    def _save_metadata(self, metadata_path, content_size, content_type="html"):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            'created_time': datetime.now().isoformat(),
            'content_size': content_size,
            'content_type': content_type,
            'expiry_days': self.cache_expiry_days
        }
        
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _cleanup_expired_cache(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        for cache_file in self.cache_dir.glob("*.cache"):
            metadata_path = cache_file.with_name(f"{cache_file.stem}_meta.json")
            if not self._is_cache_valid(metadata_path):
                try:
                    cache_file.unlink()
                    if metadata_path.exists():
                        metadata_path.unlink()
                except Exception as e:
                    print(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _check_cache_size(self):
        """ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬"""
        total_size = 0
        cache_files = []
        
        for cache_file in self.cache_dir.glob("*.cache"):
            size = cache_file.stat().st_size
            total_size += size
            cache_files.append((cache_file, size))
        
        # MBë¡œ ë³€í™˜
        total_size_mb = total_size / (1024 * 1024)
        
        if total_size_mb > self.max_cache_size_mb:
            print(f"âš ï¸  ìºì‹œ í¬ê¸° ì´ˆê³¼: {total_size_mb:.2f}MB > {self.max_cache_size_mb}MB")
            print(f"ğŸ“ ìºì‹œ íŒŒì¼ë“¤:")
            for cache_file, size in cache_files:
                try:
                    metadata_path = cache_file.with_name(f"{cache_file.stem}_meta.json")
                    cache_file.unlink()
                    if metadata_path.exists():
                        metadata_path.unlink()
                    
                    total_size_mb -= size / (1024 * 1024)
                    if total_size_mb <= self.max_cache_size_mb * 0.8:  # 80%ê¹Œì§€ ì •ë¦¬
                        break
                except Exception as e:
                    print(f"ìºì‹œ í¬ê¸° ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def get_cached_content(self, identifier, content_type="html"):
        """ìºì‹œëœ ì»¨í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
        cache_key = self._get_cache_key(identifier, content_type)
        cache_path = self._get_cache_path(cache_key)
        metadata_path = self._get_metadata_path(cache_key)
        
        # ìºì‹œê°€ ì¡´ì¬í•˜ê³  ìœ íš¨í•œì§€ í™•ì¸
        if cache_path.exists() and self._is_cache_valid(metadata_path):
            try:
                with open(cache_path, 'rb') as f:
                    content = pickle.load(f)
                return content
            except Exception as e:
                print(f"ìºì‹œ ì½ê¸° ì˜¤ë¥˜: {e}")
                return None
        
        return None
    
    def save_cached_content(self, identifier, content, content_type="html"):
        """ì»¨í…ì¸ ë¥¼ ìºì‹œì— ì €ì¥"""
        cache_key = self._get_cache_key(identifier, content_type)
        cache_path = self._get_cache_path(cache_key)
        metadata_path = self._get_metadata_path(cache_key)
        
        try:
            print(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì¤‘: {cache_key}")
            
            # ì»¨í…ì¸ ë¥¼ pickleë¡œ ì €ì¥
            with open(cache_path, 'wb') as f:
                pickle.dump(content, f)
            
            # ì €ì¥ëœ íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = cache_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            print(f"ğŸ“ ì €ì¥ëœ íŒŒì¼ í¬ê¸°: {file_size_mb:.2f}MB")
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            content_size = len(str(content).encode('utf-8'))
            self._save_metadata(metadata_path, content_size, content_type)
            
            # ìºì‹œ í¬ê¸° í™•ì¸ ë° ì •ë¦¬ (ì €ì¥ í›„ì—ë§Œ)
            self._check_cache_size()
            
            return True
        except Exception as e:
            print(f"âŒ ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def invalidate_cache(self, identifier, content_type="html"):
        """íŠ¹ì • ìºì‹œ ë¬´íš¨í™”"""
        cache_key = self._get_cache_key(identifier, content_type)
        cache_path = self._get_cache_path(cache_key)
        metadata_path = self._get_metadata_path(cache_key)
        
        try:
            if cache_path.exists():
                cache_path.unlink()
            if metadata_path.exists():
                metadata_path.unlink()
            return True
        except Exception as e:
            print(f"ìºì‹œ ë¬´íš¨í™” ì˜¤ë¥˜: {e}")
            return False
    
    def clear_all_cache(self):
        """ëª¨ë“  ìºì‹œ ì •ë¦¬"""
        try:
            for cache_file in self.cache_dir.glob("*"):
                cache_file.unlink()
            return True
        except Exception as e:
            print(f"ì „ì²´ ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
            return False
    
    def get_cache_info(self):
        """ìºì‹œ ì •ë³´ ë°˜í™˜"""
        cache_files = list(self.cache_dir.glob("*.cache"))
        total_size = sum(f.stat().st_size for f in cache_files)
        total_size_mb = total_size / (1024 * 1024)
        
        # ìºì‹œ íŒŒì¼ ìƒì„¸ ì •ë³´
        cache_details = []
        for cache_file in cache_files:
            metadata_path = cache_file.with_name(f"{cache_file.stem}_meta.json")
            file_info = {
                'filename': cache_file.name,
                'size_mb': round(cache_file.stat().st_size / (1024 * 1024), 3),
                'created_time': None,
                'content_type': None
            }
            
            if metadata_path.exists():
                try:
                    with open(metadata_path, 'r') as f:
                        metadata = json.load(f)
                    file_info['created_time'] = metadata.get('created_time')
                    file_info['content_type'] = metadata.get('content_type')
                except Exception:
                    pass
            
            cache_details.append(file_info)
        
        return {
            'total_files': len(cache_files),
            'total_size_mb': round(total_size_mb, 2),
            'max_size_mb': self.max_cache_size_mb,
            'expiry_days': self.cache_expiry_days,
            'cache_dir': str(self.cache_dir.absolute()),
            'cache_details': cache_details
        }

def get_cache_manager(dataset_directory):
    """ë°ì´í„°ì…‹ë³„ ìºì‹œ ë§¤ë‹ˆì € ë°˜í™˜ (dataset_directory í•„ìˆ˜)"""
    if not dataset_directory:
        raise ValueError("dataset_directoryëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. ì „ì—­ ìºì‹œëŠ” ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    return CacheManager(dataset_directory)


def get_cache_repository(dataset_directory, dataset_name: Optional[str] = None) -> CacheRepository:
    """ì¤‘ì•™ ì €ì¥ì†Œ ìºì‹œ ë§¤ë‹ˆì € ë°˜í™˜"""

    return CacheRepository.from_directory(dataset_directory, dataset_name=dataset_name)

def get_cached_html_content(identifier, generator_func, *args, dataset_directory):
    """HTML ì»¨í…ì¸ ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„±"""
    cache_manager = get_cache_manager(dataset_directory)
    
    # ìºì‹œì—ì„œ í™•ì¸
    cached_content = cache_manager.get_cached_content(identifier, "html")
    if cached_content is not None:
        return cached_content
    
    # ìºì‹œì— ì—†ìœ¼ë©´ ìƒì„±
    try:
        content = generator_func(*args)
        cache_manager.save_cached_content(identifier, content, "html")
        return content
    except Exception as e:
        # ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ ìºì‹œì— ì €ì¥í•˜ì§€ ì•Šê³  ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œí‚´
        raise e


def _parse_created_time(meta_path: Path) -> datetime:
    try:
        with open(meta_path, 'r') as f:
            metadata = json.load(f)
        return datetime.fromisoformat(metadata.get('created_time'))
    except Exception:
        # fallback to file mtime
        try:
            return datetime.fromtimestamp(meta_path.stat().st_mtime)
        except Exception:
            return datetime.min


def get_latest_cached_content_by_prefix(directory, identifier_prefix: str, exclude_stem: str | None = None):
    """ê°€ì¥ ìµœê·¼ ë©”íƒ€ ê¸°ì¤€ìœ¼ë¡œ identifier_prefixì— í•´ë‹¹í•˜ëŠ” ìºì‹œ ë‚´ìš©ì„ ë¡œë“œ

    Args:
        directory: ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬
        identifier_prefix: ìºì‹œ ì‹ë³„ì ì ‘ë‘ì–´ (ì˜ˆ: "attribute_analysis_test_data_v")
        exclude_stem: ì œì™¸í•  ìºì‹œ stem (ì˜ˆ: í˜„ì¬ ë²„ì „ì˜ ì •í™•í•œ stem)
    Returns:
        Tuple[content, stem] ë˜ëŠ” (None, None)
    """
    cache_manager = get_cache_manager(directory)
    candidates = []
    for cache_file in cache_manager.cache_dir.glob("*.cache"):
        stem = cache_file.stem
        if not stem.startswith(identifier_prefix):
            continue
        if exclude_stem and stem == exclude_stem:
            continue
        meta_path = cache_file.with_name(f"{stem}_meta.json")
        created = _parse_created_time(meta_path)
        candidates.append((created, cache_file, meta_path, stem))

    if not candidates:
        return None, None

    candidates.sort(key=lambda x: x[0], reverse=True)
    _, cache_file, _, stem = candidates[0]
    try:
        with open(cache_file, 'rb') as f:
            content = pickle.load(f)
        return content, stem
    except Exception:
        return None, None


def _build_identifier(dir_name: str, data_type: str, version: Optional[str] = None) -> str:
    return f"{data_type}_{dir_name}_{version}" if version else f"{data_type}_{dir_name}"

def _validate_cache_integrity(directory, cached_data, data_type):
    """ìºì‹œ ë°ì´í„°ì˜ ë¬´ê²°ì„±ì„ ê²€ì¦"""
    directory = Path(directory)
    
    # í˜„ì¬ ì‹¤ì œ íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    formats = ('.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG')
    current_files = set()
    
    # ì§ì ‘ íŒŒì¼ë“¤
    for fmt in formats:
        current_files.update(f.name for f in directory.glob(f"*{fmt}"))
    
    # YOLO êµ¬ì¡° íŒŒì¼ë“¤
    yolo_subdirs = ['train/images', 'valid/images', 'test/images']
    for subdir in yolo_subdirs:
        subdir_path = directory / subdir
        if subdir_path.exists():
            for fmt in formats:
                current_files.update(f.name for f in subdir_path.glob(f"*{fmt}"))
    
    # ìºì‹œëœ íŒŒì¼ ëª©ë¡
    cached_files = set(cached_data.keys())
    
    # íŒŒì¼ ëª©ë¡ ë¹„êµ
    if current_files != cached_files:
        print(f"ğŸ“Š File mismatch detected:")
        print(f"   Current files: {len(current_files)}")
        print(f"   Cached files: {len(cached_files)}")
        if current_files - cached_files:
            print(f"   Added: {len(current_files - cached_files)} files")
        if cached_files - current_files:
            print(f"   Removed: {len(cached_files - current_files)} files")
        return False
    
    # ë‚´ìš© ë³€ê²½ ê°ì§€ (ê°€ëŠ¥í•œ ê²½ìš°ì— í•œí•¨)
    try:
        # íŒŒì¼ í¬ê¸°/mtime ê¸°ë°˜ ë³€ê²½ ê°ì§€: attribute_analysisëŠ” size(MB)ë¥¼ ë³´ê´€í•¨
        if data_type == "attribute_analysis":
            for fname in cached_files:
                cached_entry = cached_data.get(fname, {})
                cached_size_mb = cached_entry.get('size')
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ íƒìƒ‰ (flat + YOLO)
                file_path = None
                candidates = [directory / fname,
                              directory / 'train/images' / fname,
                              directory / 'valid/images' / fname,
                              directory / 'test/images' / fname]
                for c in candidates:
                    if c.exists():
                        file_path = c
                        break
                if not file_path:
                    continue
                real_size_mb = file_path.stat().st_size / (1024 * 1024)
                if cached_size_mb is not None and abs(real_size_mb - float(cached_size_mb)) > 0.01:
                    print(f"ğŸ”„ Detected content change in '{fname}' (size diff). Invalidating cache.")
                    return False
        # embedding_analysisëŠ” íŒŒì¼ ëª©ë¡ë§Œ ë³´ê´€ â†’ ë‚´ìš© ë³€ê²½ì€ ìƒìœ„ ë¡œì§ì—ì„œ ì²˜ë¦¬
    except Exception:
        # ë³´ìˆ˜ì ìœ¼ë¡œ í†µê³¼
        pass

    return True

def get_cached_analysis_data(directory, data_type="image_analysis"):
    """ë¶„ì„ ë°ì´í„°ë¥¼ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ìƒì„± (ë¬´ê²°ì„± ê²€ì¦ í¬í•¨)"""
    cache_manager = get_cache_manager(directory)
    
    identifier = _build_identifier(os.path.basename(directory), data_type)
    
    # ìºì‹œì—ì„œ í™•ì¸ (content_typeì„ ë¹ˆ ë¬¸ìì—´ë¡œ í•˜ì—¬ ì ‘ë‘ì‚¬ ì—†ì´ ë¡œë“œ)
    cached_data = cache_manager.get_cached_content(identifier, "")
    if cached_data is not None:
        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        if _validate_cache_integrity(directory, cached_data, data_type):
            return cached_data
        else:
            print(f"âš ï¸ Cache validation failed for {identifier}, invalidating...")
            cache_manager.invalidate_cache(identifier, "")
            return None
    
    return None

def get_cached_analysis_data_by_version(directory, data_type="image_analysis", version=None):
    """ë²„ì „ ì •ë³´ë¥¼ í¬í•¨í•œ ìºì‹œ ì¡°íšŒ (ë¬´ê²°ì„± ê²€ì¦ í¬í•¨)"""
    cache_manager = get_cache_manager(directory)
    
    identifier = _build_identifier(os.path.basename(directory), data_type, version)

    cached_data = cache_manager.get_cached_content(identifier, "")
    
    if cached_data is not None:
        # ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
        if _validate_cache_integrity(directory, cached_data, data_type):
            return cached_data
        else:
            print(f"âš ï¸ Cache validation failed for {identifier}, invalidating...")
            cache_manager.invalidate_cache(identifier, "")
            return None
    
    return None

def save_analysis_data(directory, data, data_type="image_analysis"):
    """ë¶„ì„ ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
    cache_manager = get_cache_manager(directory)

    identifier = _build_identifier(os.path.basename(directory), data_type)

    # content_typeì„ ë¹ˆ ë¬¸ìì—´ë¡œ í•˜ì—¬ ì ‘ë‘ì‚¬ ì—†ì´ ì €ì¥
    return cache_manager.save_cached_content(identifier, data, "")

def save_analysis_data_by_version(directory, data, data_type="image_analysis", version=None):
    """ë²„ì „ ì •ë³´ë¥¼ í¬í•¨í•œ ìºì‹œ ì €ì¥"""
    cache_manager = get_cache_manager(directory)

    identifier = _build_identifier(os.path.basename(directory), data_type, version)

    return cache_manager.save_cached_content(identifier, data, "")


def export_local_cache_to_repository(
    dataset_directory: str | Path,
    version: Optional[str],
    data_types: Iterable[str] = DEFAULT_CACHE_DATA_TYPES,
    dataset_name: Optional[str] = None,
) -> Dict[str, Path]:
    """í˜„ì¬ ë¡œì»¬ ìºì‹œë¥¼ ì¤‘ì•™ ì €ì¥ì†Œë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""

    cache_manager = get_cache_manager(dataset_directory)
    repository = get_cache_repository(dataset_directory, dataset_name=dataset_name)
    # Use directory basename for identifier (matches how cache was saved)
    dir_name = os.path.basename(dataset_directory)
    saved_paths: Dict[str, Path] = {}

    for data_type in data_types:
        identifier = _build_identifier(dir_name, data_type, version)
        content = cache_manager.get_cached_content(identifier, "")
        if content is None:
            continue
        saved_paths[data_type] = repository.save(version or "unknown", data_type, content)

    return saved_paths


def import_repository_cache_to_local(
    dataset_directory: str | Path,
    version: Optional[str],
    data_types: Iterable[str] = DEFAULT_CACHE_DATA_TYPES,
    *,
    clear_existing: bool = False,
    dataset_name: Optional[str] = None,
) -> Dict[str, bool]:
    """ì¤‘ì•™ ì €ì¥ì†Œ ìºì‹œë¥¼ ë¡œì»¬ ìºì‹œ ë””ë ‰í† ë¦¬ì— ë³µì›í•©ë‹ˆë‹¤."""

    cache_manager = get_cache_manager(dataset_directory)
    repository = get_cache_repository(dataset_directory, dataset_name=dataset_name)

    if clear_existing:
        cache_manager.clear_all_cache()

    # Use directory basename for identifier (matches how cache is saved)
    dir_name = os.path.basename(dataset_directory)
    restored: Dict[str, bool] = {}

    for data_type in data_types:
        content = repository.load(version or "unknown", data_type)
        if content is None:
            restored[data_type] = False
            continue
        identifier = _build_identifier(dir_name, data_type, version)
        cache_manager.save_cached_content(identifier, content, "")
        restored[data_type] = True

    return restored


def repository_has_cache(
    dataset_directory: str | Path, version: Optional[str], data_type: str
) -> bool:
    repository = get_cache_repository(dataset_directory)
    cache_path = repository.version_dir(version or "unknown") / f"{data_type}.cache"
    return cache_path.exists()

 