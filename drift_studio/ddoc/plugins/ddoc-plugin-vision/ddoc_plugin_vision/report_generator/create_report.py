import json
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import base64
from io import BytesIO
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

# ìºì‹œ ë§¤ë‹ˆì € import
import sys
import os
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
from cache_utils.cache_manager import get_cached_analysis_data

# ì°¨íŠ¸ ì„¤ëª… ìƒì„±ê¸° import
try:
    from .guideline_generator import ChartDescriptionGenerator
except ImportError:
    try:
        from guideline_generator import ChartDescriptionGenerator
    except ImportError:
        ChartDescriptionGenerator = None

# ì°¨íŠ¸ ì„¤ëª… ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chart_description_generator = ChartDescriptionGenerator() if ChartDescriptionGenerator else None

def add_chart_description_to_report(chart_key, chart_html):
    """ì°¨íŠ¸ì— ì„¤ëª…ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    if not chart_description_generator:
        return chart_html
        
    description_html = chart_description_generator.generate_description_html(chart_key)
    if description_html:
        return f"""
        <div class="chart-with-description">
            {chart_html}
            {description_html}
        </div>
        """
    return chart_html

def add_chart_descriptions_to_section(section_html, chart_keys):
    """ì„¹ì…˜ ë‚´ì˜ ì°¨íŠ¸ë“¤ì— ì„¤ëª…ì„ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜"""
    if not chart_description_generator or not chart_keys:
        return section_html
    
    # ê° ì°¨íŠ¸ í‚¤ì— ëŒ€í•´ ì„¤ëª… ì¶”ê°€
    for chart_key in chart_keys:
        if chart_key in chart_description_generator.descriptions:
            # ì°¨íŠ¸ ì œëª©ì„ ì°¾ì•„ì„œ ì„¤ëª… ì¶”ê°€
            title_pattern = f'<h[1-6][^>]*>{chart_description_generator.descriptions[chart_key]["title"]}</h[1-6]>'
            import re
            if re.search(title_pattern, section_html):
                section_html = re.sub(
                    title_pattern,
                    f'\\g<0>{chart_description_generator.generate_description_html(chart_key)}',
                    section_html
                )
    
    return section_html

# XAI ê°€ì´ë“œë¼ì¸ ìƒì„±ê¸° import
try:
    from .xai_guideline_generator import create_xai_guideline
except ImportError:
    # ìƒëŒ€ importê°€ ì‹¤íŒ¨í•  ê²½ìš° ì ˆëŒ€ import ì‹œë„
    try:
        from xai_guideline_generator import create_xai_guideline
    except ImportError:
        create_xai_guideline = None

'''
ğŸ“Š {dataset_name} í†µí•© ë¶„ì„ ë¦¬í¬íŠ¸
â”œâ”€â”€ ğŸ“Š Dataset Information & Statistics (ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ê²°ê³¼)
â”œâ”€â”€ ğŸ–¼ï¸ Image Analysis Results (ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼)
â”‚   â”œâ”€â”€ ğŸ“ˆ Summary Statistics
â”‚   â”œâ”€â”€ ğŸ“‹ Format Distribution
â”‚   â”œâ”€â”€ ğŸ“Š Visualizations
â”‚   â”œâ”€â”€ ğŸ–¼ï¸ Sample Images
â”‚   â”œâ”€â”€ ğŸ“Š Detailed Statistics
â”‚   â”œâ”€â”€ ğŸ§  Embedding Information
â”‚   â””â”€â”€ ğŸ“ Resolution Information
â””â”€â”€ ğŸ” Data Drift Analysis Results (ë“œë¦¬í”„íŠ¸ ë¶„ì„ ê²°ê³¼)
'''

class ImageAnalysisReport:
    def __init__(self, directory):
        self.directory = directory
        # ë°ì´í„°ë¥¼ ì»¨í…ì¸ ë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ ì €ì¥
        self.attr_data = self.load_attribute_data()
        self.embed_data = self.load_embedding_data()
        self.xai_data = self.load_xai_data()
        self.clustering_data = self.load_clustering_data()
        
        # ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³‘í•© ë°ì´í„° (í•„ìš”ì‹œì—ë§Œ ì‚¬ìš©)
        self.data = self._merge_data_for_compatibility()
        
    def load_attribute_data(self):
        """ì†ì„± ë¶„ì„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        cached_data = get_cached_analysis_data(self.directory, "attribute_analysis")
        if cached_data is not None:
            print(f"ğŸ“Š Loaded attribute data: {len(cached_data)} files")
            return cached_data
            
        print("â„¹ï¸  No attribute analysis data found in cache")
        return {}
    
    def load_embedding_data(self):
        """ì„ë² ë”© ë¶„ì„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        cached_data = get_cached_analysis_data(self.directory, "embedding_analysis")
        if cached_data is not None:
            print(f"ğŸ“Š Loaded embedding data: {len(cached_data)} files")
            return cached_data
            
        print("â„¹ï¸  No embedding analysis data found in cache")
        return {}
    
    def load_clustering_data(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        # 1. ì „ìš© í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ë°ì´í„° í™•ì¸
        cached_data = get_cached_analysis_data(self.directory, "clustering_analysis")
        if cached_data is not None:
            print(f"ğŸ“Š Loaded clustering data : ")
            return cached_data
        
        # 2. ì„ë² ë”© ë¶„ì„ ê²°ê³¼ì—ì„œ í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì¶”ì¶œ
        cached_data = get_cached_analysis_data(self.directory, "embedding_analysis")
        if cached_data is not None:
            # í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì¶”ì¶œ
            clustering_data = {}
            for filename, data in cached_data.items():
                if isinstance(data, dict) and 'cluster' in data:
                    clustering_data[filename] = {
                        'cluster': data['cluster'],
                        'embedding': data.get('embedding', None)
                    }
            
            if clustering_data:
                print(f"ğŸ“Š Loaded clustering data from embedding analysis: {len(clustering_data)} files")
                return clustering_data
            
        print("â„¹ï¸  No clustering data found in cache")
        return {}
    
    def load_xai_data(self):
        """XAI ë¶„ì„ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        cached_data = get_cached_analysis_data(self.directory, "xai_analysis")
        if cached_data is not None:
            print(f"ğŸ“Š Loaded XAI data: {len(cached_data)} files")
            return cached_data
        print("â„¹ï¸  No XAI analysis data found in cache")
        return {}
    
    def _merge_data_for_compatibility(self):
        """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë³‘í•©í•©ë‹ˆë‹¤."""
        merged_data = self.attr_data.copy()
        
        # ì„ë² ë”© ë°ì´í„° ë³‘í•©
        for filename, embed_item in self.embed_data.items():
            if filename in merged_data:
                merged_data[filename].update(embed_item)
            else:
                merged_data[filename] = embed_item
        
        return merged_data
    
    def get_combined_data(self):
        """ëª¨ë“  ë¶„ì„ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
        combined_data = {}
        
        # ì†ì„± ë°ì´í„° ì¶”ê°€
        for filename, attr_item in self.attr_data.items():
            combined_data[filename] = attr_item.copy()
        
        # ì„ë² ë”© ë°ì´í„° ì¶”ê°€
        for filename, embed_item in self.embed_data.items():
            if filename in combined_data:
                combined_data[filename].update(embed_item)
            else:
                combined_data[filename] = embed_item
        
        # XAI ë°ì´í„° ì¶”ê°€
        for filename, xai_item in self.xai_data.items():
            if filename in combined_data:
                combined_data[filename]['xai_analysis'] = xai_item
            else:
                combined_data[filename] = {'xai_analysis': xai_item}
        
        return combined_data
    
    def create_summary_stats(self):
        """ê¸°ë³¸ í†µê³„ ì •ë³´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        # ì†ì„± ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not self.attr_data:
            return {}
        
        total_images = len(self.attr_data)
        total_size = sum(item['size'] for item in self.attr_data.values())
        
        # í˜•ì‹ë³„ í†µê³„
        formats = {}
        resolutions = {}
        sizes = []
        noise_levels = []
        sharpness_values = []
        
        for item in self.attr_data.values():
            # í˜•ì‹ë³„ ì¹´ìš´íŠ¸
            fmt = item['format']
            formats[fmt] = formats.get(fmt, 0) + 1
            
            # í•´ìƒë„ë³„ ì¹´ìš´íŠ¸
            res = item['resolution']
            resolutions[res] = resolutions.get(res, 0) + 1
            
            # ìˆ˜ì¹˜ ë°ì´í„°
            sizes.append(item['size'])
            noise_levels.append(item['noise_level'])
            sharpness_values.append(item['sharpness'])
        
        return {
            'total_images': total_images,
            'total_size_mb': total_size,
            'avg_size_mb': np.mean(sizes),
            'formats': formats,
            'resolutions': resolutions,
            'size_stats': {
                'min': np.min(sizes),
                'max': np.max(sizes),
                'mean': np.mean(sizes),
                'std': np.std(sizes)
            },
            'noise_stats': {
                'min': np.min(noise_levels),
                'max': np.max(noise_levels),
                'mean': np.mean(noise_levels),
                'std': np.std(noise_levels)
            },
            'sharpness_stats': {
                'min': np.min(sharpness_values),
                'max': np.max(sharpness_values),
                'mean': np.mean(sharpness_values),
                'std': np.std(sharpness_values)
            }
        }
    
    def create_basic_attribute_charts(self):
        """ê¸°ë³¸ ì†ì„± ê´€ë ¨ ì°¨íŠ¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        charts = {}
        
        # 1. íŒŒì¼ í¬ê¸° ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        sizes = [item['size'] for item in self.data.values()]
        plt.figure(figsize=(10, 6))
        plt.hist(sizes, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        plt.title('File Size Distribution', fontsize=14, fontweight='bold')
        plt.xlabel('File Size (MB)')
        plt.ylabel('Frequency')
        plt.grid(True, alpha=0.3)
        charts['size_distribution'] = self.fig_to_base64()
        plt.close()
        
        # 2. í˜•ì‹ë³„ ë¶„í¬ íŒŒì´ ì°¨íŠ¸
        formats = {}
        for item in self.data.values():
            fmt = item['format']
            formats[fmt] = formats.get(fmt, 0) + 1
        
        if formats:
            plt.figure(figsize=(8, 8))
            plt.pie(formats.values(), labels=formats.keys(), autopct='%1.1f%%', startangle=90)
            plt.title('Image Format Distribution', fontsize=14, fontweight='bold')
            charts['format_distribution'] = self.fig_to_base64()
            plt.close()
        
        # 3. í•´ìƒë„ë³„ ë¶„í¬ (ìƒìœ„ 10ê°œ)
        resolutions = {}
        for item in self.data.values():
            res = item['resolution']
            resolutions[res] = resolutions.get(res, 0) + 1
        
        top_resolutions = dict(sorted(resolutions.items(), key=lambda x: x[1], reverse=True)[:10])
        
        if top_resolutions:
            plt.figure(figsize=(12, 6))
            plt.bar(range(len(top_resolutions)), list(top_resolutions.values()), color='lightgreen')
            plt.title('Resolution Distribution (Top 10)', fontsize=14, fontweight='bold')
            plt.xlabel('Resolution')
            plt.ylabel('Count')
            plt.xticks(range(len(top_resolutions)), list(top_resolutions.keys()), rotation=45, ha='right')
            plt.grid(True, alpha=0.3)
            charts['resolution_distribution'] = self.fig_to_base64()
            plt.close()
        
        return charts
    
    def create_quality_attribute_charts(self):
        """ì´ë¯¸ì§€ í’ˆì§ˆ ì†ì„± ê´€ë ¨ ì°¨íŠ¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        charts = {}
        
        # ë…¸ì´ì¦ˆ vs ì„ ëª…ë„ ì‚°ì ë„
        noise_levels = [item['noise_level'] for item in self.data.values()]
        sharpness_values = [item['sharpness'] for item in self.data.values()]
        
        plt.figure(figsize=(10, 6))
        plt.scatter(noise_levels, sharpness_values, alpha=0.6, color='coral')
        plt.title('Noise Level vs Edgeness', fontsize=14, fontweight='bold')
        plt.xlabel('Noise Level')
        plt.ylabel('Edgeness')
        plt.grid(True, alpha=0.3)
        charts['noise_vs_sharpness'] = self.fig_to_base64()
        plt.close()
        
        return charts
    
    def create_embedding_charts(self):
        """ì„ë² ë”© ë¶„ì„ ê´€ë ¨ ì°¨íŠ¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        charts = {}
        
        # ì„ë² ë”© ê³µê°„ ì‹œê°í™” (PCA)
        if self.embed_data and len(self.embed_data) > 1:
            embeddings = np.array([item['embedding'] for item in self.embed_data.values()])
            pca = PCA(n_components=2)
            embeddings_2d = pca.fit_transform(embeddings)
            
            plt.figure(figsize=(10, 8))
            plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, color='purple')
            plt.title('Image Embeddings (PCA 2D)', fontsize=14, fontweight='bold')
            plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')
            plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')
            plt.grid(True, alpha=0.3)
            charts['embeddings_pca'] = self.fig_to_base64()
            plt.close()
        
        return charts
    
    def create_clustering_charts(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê´€ë ¨ ì°¨íŠ¸ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        charts = {}
        
        if self.clustering_data and 'embeddings_2d' in self.clustering_data:
            embeddings_2d = np.array(self.clustering_data['embeddings_2d'])
            cluster_labels = np.array(self.clustering_data['cluster_labels'])
            method = self.clustering_data.get('method', 'Unknown')
            n_clusters = self.clustering_data.get('n_clusters', 0)
            
            # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
            plt.figure(figsize=(12, 8))
            scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                                c=cluster_labels, cmap='viridis', alpha=0.6)
            
            # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì  í‘œì‹œ (í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒìœ¼ë¡œ êµ¬ë¶„)
            if 'centroids' in self.clustering_data and self.clustering_data['centroids']:
                centroids_2d = np.array(self.clustering_data['centroids'])
                
                # í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ ì •ì˜ (viridis ì»¬ëŸ¬ë§µ ì‚¬ìš©)
                cluster_colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))
                
                # 2D ì„¼íŠ¸ë¡œì´ë“œ í‘œì‹œ (í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ, X ë§ˆì»¤)
                for i in range(n_clusters):
                    plt.scatter(centroids_2d[i, 0], centroids_2d[i, 1], 
                               c=[cluster_colors[i]], marker='x', s=200, linewidths=3, 
                               label='2D Centroid' if i == 0 else "")
                
                # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œë¥¼ PCAë¡œ ì¶•ì†Œí•œ ì¢Œí‘œ í‘œì‹œ (í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ, O ë§ˆì»¤)
                if 'centroids_high_dim' in self.clustering_data and 'pca_components' in self.clustering_data:
                    try:
                        # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œ ê°€ì ¸ì˜¤ê¸°
                        centroids_high_dim = np.array(self.clustering_data['centroids_high_dim'])
                        pca_components = np.array(self.clustering_data['pca_components'])
                        
                        # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œë¥¼ 2Dë¡œ ì¶•ì†Œ
                        centroids_high_dim_2d = np.dot(centroids_high_dim, pca_components.T)
                        
                        # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œ PCA ì¶•ì†Œ ì¢Œí‘œ í‘œì‹œ (í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ, O ë§ˆì»¤)
                        for i in range(n_clusters):
                            plt.scatter(centroids_high_dim_2d[i, 0], centroids_high_dim_2d[i, 1], 
                                       c=[cluster_colors[i]], marker='o', s=150, linewidths=2, alpha=0.8,
                                       label='High-Dim Centroid' if i == 0 else "")
                        
                        # í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ì™€ ê±°ë¦¬ ì •ë³´ë¥¼ í´ëŸ¬ìŠ¤í„° ì™¸ê³½ì— í‘œì‹œ
                        for i in range(n_clusters):
                            # 2D ì„¼íŠ¸ë¡œì´ë“œì™€ ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œ PCA ì¶•ì†Œ ì¢Œí‘œ ê°„ì˜ ê±°ë¦¬ ê³„ì‚°
                            distance = np.linalg.norm(centroids_2d[i] - centroids_high_dim_2d[i])
                            
                            # ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€ê²½
                            if distance > 0.5:
                                text_color = 'red'  # í° ì°¨ì´
                                alpha = 0.9
                            elif distance > 0.2:
                                text_color = 'orange'  # ì¤‘ê°„ ì°¨ì´
                                alpha = 0.8
                            else:
                                text_color = 'green'  # ì‘ì€ ì°¨ì´
                                alpha = 0.7
                            
                            # ë‘ ì„¼íŠ¸ë¡œì´ë“œë¥¼ ì—°ê²°í•˜ëŠ” ì„  ê·¸ë¦¬ê¸° (í´ëŸ¬ìŠ¤í„°ë³„ ìƒ‰ìƒ)
                            plt.plot([centroids_2d[i, 0], centroids_high_dim_2d[i, 0]], 
                                   [centroids_2d[i, 1], centroids_high_dim_2d[i, 1]], 
                                   color=cluster_colors[i], linestyle='--', alpha=0.6, linewidth=1.5)
                            
                            # ê±°ë¦¬ ì •ë³´ë¥¼ ì ì„ ì˜ ì¤‘ê°„ ì§€ì ì— í‘œì‹œ
                            mid_x = (centroids_2d[i, 0] + centroids_high_dim_2d[i, 0]) / 2
                            mid_y = (centroids_2d[i, 1] + centroids_high_dim_2d[i, 1]) / 2
                            
                            # ê±°ë¦¬ ì •ë³´ í…ìŠ¤íŠ¸ í‘œì‹œ (ì ì„  ì¤‘ê°„ì—)
                            plt.annotate(f'd={distance:.3f}', 
                                       (mid_x, mid_y),
                                       xytext=(0, 0), textcoords='offset points',
                                       fontsize=9, color=text_color, fontweight='bold',
                                       bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=alpha, edgecolor=cluster_colors[i], linewidth=1.0),
                                       ha='center', va='center')
                        
                        # í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ 2D ì„¼íŠ¸ë¡œì´ë“œ ì§€ì ì— ì›Œí„°ë§ˆí¬ì²˜ëŸ¼ í‘œì‹œ
                        for i in range(n_clusters):
                            plt.annotate(f'C{i}', 
                                       (centroids_2d[i, 0], centroids_2d[i, 1]),
                                       xytext=(0, 0), textcoords='offset points',
                                       fontsize=45, color=cluster_colors[i], fontweight='bold',
                                       alpha=0.3,  # ë‚®ì€ íˆ¬ëª…ë„ë¡œ ì›Œí„°ë§ˆí¬ íš¨ê³¼
                                       ha='center', va='center')
                        
                        title_text = f'Clustering Results - {method.upper()} ({n_clusters} clusters)\nSame color = Same cluster'
                        
                    except Exception as e:
                        print(f"âš ï¸ Error projecting high-dimensional centroids: {e}")
                        # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œ íˆ¬ì˜ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ 2D ì„¼íŠ¸ë¡œì´ë“œë§Œ í‘œì‹œ
                        for i in range(n_clusters):
                            plt.scatter(centroids_2d[i, 0], centroids_2d[i, 1], 
                                       c=[cluster_colors[i]], marker='x', s=200, linewidths=3, 
                                       label=f'C{i} Centroid' if i == 0 else "")
                        
                        # í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ 2D ì„¼íŠ¸ë¡œì´ë“œ ì§€ì ì— ì›Œí„°ë§ˆí¬ì²˜ëŸ¼ í‘œì‹œ
                        for i in range(n_clusters):
                            plt.annotate(f'C{i}', 
                                       (centroids_2d[i, 0], centroids_2d[i, 1]),
                                       xytext=(0, 0), textcoords='offset points',
                                       fontsize=45, color=cluster_colors[i], fontweight='bold',
                                       alpha=0.3,  # ë‚®ì€ íˆ¬ëª…ë„ë¡œ ì›Œí„°ë§ˆí¬ íš¨ê³¼
                                       ha='center', va='center')
                        
                        title_text = f'Clustering Results - {method.upper()} ({n_clusters} clusters)\n2D Density-Weighted Centroids (X markers)'
                else:
                    # ê³ ì°¨ì› ì„¼íŠ¸ë¡œì´ë“œ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ í‘œì‹œ
                    cluster_colors = plt.cm.viridis(np.linspace(0, 1, n_clusters))
                    
                    for i in range(n_clusters):
                        plt.scatter(centroids_2d[i, 0], centroids_2d[i, 1], 
                                   c=[cluster_colors[i]], marker='x', s=200, linewidths=3, 
                                   label=f'C{i} Centroid' if i == 0 else "")
                    
                    # í´ëŸ¬ìŠ¤í„° ë²ˆí˜¸ë¥¼ 2D ì„¼íŠ¸ë¡œì´ë“œ ì§€ì ì— ì›Œí„°ë§ˆí¬ì²˜ëŸ¼ í‘œì‹œ
                    for i in range(n_clusters):
                        plt.annotate(f'C{i}', 
                                   (centroids_2d[i, 0], centroids_2d[i, 1]),
                                   xytext=(0, 0), textcoords='offset points',
                                   fontsize=45, color=cluster_colors[i], fontweight='bold',
                                   alpha=0.3,  # ë‚®ì€ íˆ¬ëª…ë„ë¡œ ì›Œí„°ë§ˆí¬ íš¨ê³¼
                                   ha='center', va='center')
                    
                    title_text = f'Clustering Results - {method.upper()} ({n_clusters} clusters)\n2D Density-Weighted Centroids (X markers)'
            
            plt.title(title_text, fontsize=14, fontweight='bold')
            plt.xlabel('PC1')
            plt.ylabel('PC2')
            plt.colorbar(scatter, label='Cluster')
            plt.legend()
            plt.grid(True, alpha=0.3)
            charts['clustering_results'] = self.fig_to_base64()
            plt.close()
            
            # í´ëŸ¬ìŠ¤í„° í¬ê¸° ë¶„í¬
            if 'cluster_stats' in self.clustering_data:
                cluster_sizes = []
                cluster_names = []
                for i in range(n_clusters):
                    cluster_key = f'cluster_{i}'
                    if cluster_key in self.clustering_data['cluster_stats']:
                        cluster_sizes.append(self.clustering_data['cluster_stats'][cluster_key]['size'])
                        cluster_names.append(f'Cluster {i}')
                
                plt.figure(figsize=(10, 6))
                plt.bar(cluster_names, cluster_sizes, color='lightcoral', alpha=0.7)
                plt.title('Cluster Size Distribution', fontsize=14, fontweight='bold')
                plt.xlabel('Cluster')
                plt.ylabel('Number of Images')
                plt.xticks(rotation=45)
                plt.grid(True, alpha=0.3)
                
                # ê°’ í‘œì‹œ
                for i, v in enumerate(cluster_sizes):
                    plt.text(i, v + max(cluster_sizes) * 0.01, str(v), ha='center', va='bottom')
                
                charts['cluster_size_distribution'] = self.fig_to_base64()
                plt.close()
        
        return charts
    
    def create_clustering_summary(self):
        """í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ê²°ê³¼ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.clustering_data:
            return {}
        
        method = self.clustering_data.get('method', 'Unknown')
        n_clusters = self.clustering_data.get('n_clusters', 0)
        total_samples = len(self.clustering_data.get('file_names', []))
        
        cluster_stats = self.clustering_data.get('cluster_stats', {})
        cluster_summary = []
        
        for i in range(n_clusters):
            cluster_key = f'cluster_{i}'
            if cluster_key in cluster_stats:
                size = cluster_stats[cluster_key]['size']
                percentage = (size / total_samples * 100) if total_samples > 0 else 0
                cluster_summary.append({
                    'cluster_id': i,
                    'size': size,
                    'percentage': percentage,
                    'sample_files': cluster_stats[cluster_key]['files'][:5]  # ìƒìœ„ 5ê°œ íŒŒì¼ë§Œ
                })
        
        return {
            'method': method,
            'n_clusters': n_clusters,
            'total_samples': total_samples,
            'cluster_summary': cluster_summary
        }
    
    def create_xai_visualizations(self):
        """XAI ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. (í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ì´ë¯¸ì§€ë§Œ) - ìµœì í™”ë¨"""
        if not self.xai_data:
            print("â„¹ï¸  No XAI data found. XAI visualizations skipped.")
            return {}
        
        try:
            # ìƒëŒ€ ê²½ë¡œë¡œ import (report_generatorì—ì„œ data_utilsë¡œ ì ‘ê·¼)
            import sys
            import os
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            if project_root not in sys.path:
                sys.path.insert(0, project_root)
            from data_utils.xai_visualizer import XAIVisualizer
            
            visualizer = XAIVisualizer()
            
            # í´ëŸ¬ìŠ¤í„°ë³„ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
            representative_images = self._select_representative_images()
            
            if not representative_images:
                print("â„¹ï¸  No representative images found. Skipping XAI visualizations.")
                return {}
            
            print(f"ğŸ” Processing {len(representative_images)} representative XAI analysis results...")
            
            # ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
            xai_batch = []
            for filename in representative_images:
                if filename in self.xai_data:
                    xai_result = self.xai_data[filename]
                    if isinstance(xai_result, dict):
                        xai_batch.append((filename, xai_result))
                        print(f"  ğŸ“‹ Added {filename} to batch processing")
                    else:
                        print(f"  âš ï¸  Skipping {filename}: not a dictionary")
                else:
                    print(f"  âš ï¸  Representative image {filename} not found in XAI data")
            
            if not xai_batch:
                print("â„¹ï¸  No valid XAI data for batch processing.")
                return {}
            
            # ë°°ì¹˜ ì‹œê°í™” ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬)
            print(f"ğŸ”„ Starting batch visualization for {len(xai_batch)} images...")
            visualizations = visualizer.create_comprehensive_visualization_batch(xai_batch)
            
            print(f"ğŸ¨ Generated {len(visualizations)} XAI visualizations from {len(representative_images)} representative image")
            return visualizations
            
        except ImportError:
            print("Warning: xai_visualizer module not found. XAI visualization skipped.")
            return {}
        except Exception as e:
            print(f"Error creating XAI visualizations: {e}")
            return {}
    
    def _select_representative_images(self):
        """ì „ì²´ ìƒ˜í”Œ ì¤‘ í•˜ë‚˜ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•©ë‹ˆë‹¤."""
        representative_images = []
        
        print(f"  ğŸ” Selecting single representative image from all samples...")
        
        # XAI ë°ì´í„°ê°€ ìˆëŠ” íŒŒì¼ë“¤ë§Œ í•„í„°ë§
        if self.xai_data:
            xai_available_files = list(self.xai_data.keys())
            print(f"    - Found {len(xai_available_files)} files with XAI data")
            
            if xai_available_files:
                # ë¬´ì‘ìœ„ë¡œ í•˜ë‚˜ì˜ ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒ
                import random
                selected_file = random.choice(xai_available_files)
                representative_images.append(selected_file)
                print(f"  ğŸ“Š Selected single representative image: {selected_file}")
                print(f"  ğŸ“Š Total XAI files available: {len(xai_available_files)}")
            else:
                print(f"  âš ï¸  No XAI data available for representative selection")
        else:
            print(f"  âš ï¸  No XAI data available for representative selection")
        
        return representative_images
    
    def create_xai_summary_stats(self):
        """XAI ë¶„ì„ ê²°ê³¼ ìš”ì•½ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.xai_data:
            return {}
        
        total_xai_files = len(self.xai_data)
        
        # ë¶„ì„ í’ˆì§ˆ ì§€í‘œ
        quality_metrics = {
            'high_quality_analyses': 0,  # IoU > 0.5
            'medium_quality_analyses': 0,  # 0.3 < IoU <= 0.5
            'low_quality_analyses': 0,  # IoU <= 0.3
            'no_detection_analyses': 0,  # IoU = 0
            'high_entropy_analyses': 0,  # Shannon entropy > 2.0
            'low_entropy_analyses': 0,  # Shannon entropy <= 1.0
            'complex_components': 0,  # > 5 connected components
            'simple_components': 0,  # <= 2 connected components
        }
        
        # ê²€ì¶œëœ ê°ì²´ ì •ë³´
        detected_classes = {}
        model_info = {}
        
        # ëŒ€í‘œ ì´ë¯¸ì§€ ì •ë³´ (ë‹¨ì¼ ëŒ€í‘œ ì´ë¯¸ì§€)
        representative_info = {
            'total_samples': total_xai_files,
            'representative_images': 1,
            'sample_coverage': 1.0 / total_xai_files if total_xai_files > 0 else 0.0
        }
        
        for filename, xai_result in self.xai_data.items():
            if isinstance(xai_result, dict):
                # ëª¨ë¸ ì •ë³´ ìˆ˜ì§‘
                if 'model_name' in xai_result:
                    model_name = xai_result['model_name']
                    model_info[model_name] = model_info.get(model_name, 0) + 1
                
                # IoU í’ˆì§ˆ ë¶„ì„
                if 'overlap_analysis' in xai_result:
                    overlap = xai_result['overlap_analysis']
                    iou = overlap.get('iou', 0)
                    
                    if iou > 0.5:
                        quality_metrics['high_quality_analyses'] += 1
                    elif iou > 0.3:
                        quality_metrics['medium_quality_analyses'] += 1
                    elif iou > 0:
                        quality_metrics['low_quality_analyses'] += 1
                    else:
                        quality_metrics['no_detection_analyses'] += 1
                    
                    # ê²€ì¶œëœ í´ë˜ìŠ¤ ì •ë³´
                    class_name = overlap.get('largest_class_name', 'Unknown')
                    detected_classes[class_name] = detected_classes.get(class_name, 0) + 1
                
                # ì—”íŠ¸ë¡œí”¼ í’ˆì§ˆ ë¶„ì„
                if 'entropy_results' in xai_result:
                    entropy = xai_result['entropy_results']
                    shannon_entropy = entropy.get('shannon', 0)
                    
                    if shannon_entropy > 2.0:
                        quality_metrics['high_entropy_analyses'] += 1
                    elif shannon_entropy <= 1.0:
                        quality_metrics['low_entropy_analyses'] += 1
                
                # Connected Components ë³µì¡ë„ ë¶„ì„
                if 'components_analysis' in xai_result:
                    components = xai_result['components_analysis']
                    num_components = components.get('num_components', 0)
                    
                    if num_components > 5:
                        quality_metrics['complex_components'] += 1
                    elif num_components <= 2:
                        quality_metrics['simple_components'] += 1
        
        # ë‹¨ì¼ ëŒ€í‘œ ì´ë¯¸ì§€ ì •ë³´ (í´ëŸ¬ìŠ¤í„°ë§ê³¼ ë¬´ê´€í•˜ê²Œ)
        # representative_infoëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì„¤ì •ë¨
        
        # í’ˆì§ˆ ìš”ì•½
        quality_summary = {
            'excellent': quality_metrics['high_quality_analyses'],
            'good': quality_metrics['medium_quality_analyses'],
            'poor': quality_metrics['low_quality_analyses'] + quality_metrics['no_detection_analyses']
        }
        
        return {
            'total_files': total_xai_files,
            'quality_summary': quality_summary,
            'quality_metrics': quality_metrics,
            'detected_classes': detected_classes,
            'model_info': model_info,
            'representative_info': representative_info,
            'analysis_coverage': {
                'with_detections': quality_metrics['high_quality_analyses'] + quality_metrics['medium_quality_analyses'] + quality_metrics['low_quality_analyses'],
                'without_detections': quality_metrics['no_detection_analyses'],
                'high_entropy': quality_metrics['high_entropy_analyses'],
                'complex_patterns': quality_metrics['complex_components']
            }
        }
    
    def fig_to_base64(self):
        """matplotlib figureë¥¼ base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        img_str = base64.b64encode(buf.getvalue()).decode()
        buf.close()
        return img_str
    
    def create_sample_images_table(self, max_samples=10):
        """ìƒ˜í”Œ ì´ë¯¸ì§€ë“¤ì˜ ì •ë³´ë¥¼ í…Œì´ë¸”ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.attr_data:
            return []
        
        # íŒŒì¼ í¬ê¸° ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒ˜í”Œ ì„ íƒ
        sorted_items = sorted(self.attr_data.items(), key=lambda x: x[1]['size'], reverse=True)
        samples = sorted_items[:max_samples]
        
        sample_data = []
        for filename, item in samples:
            sample_data.append({
                'filename': filename,
                'size_mb': f"{item['size']:.2f}",
                'format': item['format'],
                'resolution': item['resolution'],
                'noise_level': f"{item['noise_level']:.4f}",
                'sharpness': f"{item['sharpness']:.4f}",
                'path': item['path']
            })
        
        return sample_data
    
    def generate_html_body(self):
        """report_layout.pyì— ë§ëŠ” HTML ë³¸ë¬¸ë§Œ ìƒì„±í•©ë‹ˆë‹¤."""
        if not self.attr_data and not self.embed_data and not self.xai_data:
            print("âŒ No analysis data found. Please run analysis first.")
            return None
        
        # ë°ì´í„° ì¤€ë¹„
        summary = self.create_summary_stats()
        samples = self.create_sample_images_table()
        
        # ê° ì„¹ì…˜ë³„ ì°¨íŠ¸ ìƒì„±
        basic_charts = self.create_basic_attribute_charts()
        quality_charts = self.create_quality_attribute_charts()
        embedding_charts = self.create_embedding_charts()
        clustering_charts = self.create_clustering_charts()
        
        # XAI ì‹œê°í™” ë° ìš”ì•½ í†µê³„ ìƒì„± (ëŒ€í‘œ ì´ë¯¸ì§€ ì„ íƒì€ í•œ ë²ˆë§Œ)
        xai_charts = self.create_xai_visualizations()
        xai_summary = self.create_xai_summary_stats()
        
        print(f"ğŸ“Š Report data summary:")
        print(f"  - Summary stats: {'âœ…' if summary else 'âŒ'}")
        print(f"  - Basic charts: {len(basic_charts)} charts")
        print(f"  - Quality charts: {len(quality_charts)} charts")
        print(f"  - Embedding charts: {len(embedding_charts)} charts")
        print(f"  - Clustering charts: {len(clustering_charts)} charts")
        print(f"  - Samples: {len(samples)} sample images")
        print(f"  - XAI charts: {len(xai_charts)} XAI visualizations")
        print(f"  - XAI summary: {'âœ…' if xai_summary else 'âŒ'}")
        
        # report_layout ëª¨ë“ˆì˜ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•˜ì—¬ HTML ìƒì„±
        try:
            from report_generator.report_layout import (
                generate_summary_statistics_section,
                generate_format_distribution_section,
                generate_visualizations_section,
                generate_sample_images_section,
                generate_detailed_statistics_section,
                generate_embedding_info_section,
                generate_resolution_info_section,
                generate_clustering_summary_section,
                generate_xai_analysis_section
            )
            
            # HTML íŒŒíŠ¸ë“¤ì„ ë™ì ìœ¼ë¡œ ìƒì„±
            html_parts = []
            
            # ===== ì†ì„± ë° ì„ë² ë”© ë¶„ì„ ì„¹ì…˜ ì‹œì‘ =====
            html_parts.append("""
            <div style="margin-bottom: 40px; padding: 25px; background: #f8f9fa; border-radius: 12px; border: 2px solid #e9ecef;">
                <h2 style="color: #495057; margin-bottom: 20px; padding-bottom: 12px; border-bottom: 3px solid #007bff; font-size: 1.6em;">
                    ğŸ–¼ï¸ Image Analysis Results (ì†ì„± ë° ì„ë² ë”© ë¶„ì„)
                </h2>
            """)
            
            # ===== 1. ê¸°ë³¸ ì†ì„± ë¶„ì„ =====
            # 1-1. ìš”ì•½ í†µê³„ ì„¹ì…˜ (íŒŒì¼ í¬ê¸° ì°¨íŠ¸ í¬í•¨)
            html_parts.append(generate_summary_statistics_section(summary, basic_charts.get('size_distribution')))
            
            # 1-2. ìƒ˜í”Œ ì´ë¯¸ì§€ í…Œì´ë¸” ì„¹ì…˜ (íŒŒì¼ í¬ê¸° ì°¨íŠ¸ ë°”ë¡œ ì•„ë˜)
            html_parts.append(generate_sample_images_section(samples))
            
            # 1-3. í˜•ì‹ë³„ ë¶„í¬ ì„¹ì…˜ (í˜•ì‹ë³„ ë¶„í¬ ì°¨íŠ¸ í¬í•¨)
            html_parts.append(generate_format_distribution_section(summary, basic_charts.get('format_distribution')))
            
            # 1-4. í•´ìƒë„ ì •ë³´ ì„¹ì…˜ (í•´ìƒë„ ë¶„í¬ ì°¨íŠ¸ í¬í•¨)
            html_parts.append(generate_resolution_info_section(summary, basic_charts.get('resolution_distribution')))
            
            # ===== 2. ì´ë¯¸ì§€ í’ˆì§ˆ ì†ì„± =====
            # 2-1. ìƒì„¸ í†µê³„ ì„¹ì…˜ (í’ˆì§ˆ ì†ì„± ì°¨íŠ¸ í¬í•¨)
            html_parts.append(generate_detailed_statistics_section(summary, quality_charts.get('noise_vs_sharpness')))
            
            # ===== 3. ì„ë² ë”© ë¶„ì„ =====
            # 3-1. ì„ë² ë”© ì •ë³´ ì„¹ì…˜ (ì„ë² ë”© ì°¨íŠ¸ í¬í•¨)
            html_parts.append(generate_embedding_info_section(self.embed_data, embedding_charts.get('embeddings_pca')))
            
            # ===== 4. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ =====
            # 4-1. í´ëŸ¬ìŠ¤í„°ë§ ìš”ì•½ ì„¹ì…˜ (í´ëŸ¬ìŠ¤í„°ë§ ì°¨íŠ¸ í¬í•¨)
            clustering_summary = self.create_clustering_summary()
            html_parts.append(generate_clustering_summary_section(clustering_summary, clustering_charts))
            
            # ===== ì†ì„± ë° ì„ë² ë”© ë¶„ì„ ì„¹ì…˜ ì¢…ë£Œ =====
            html_parts.append("""
            </div>
            """)
            
            # ===== XAI ë¶„ì„ ì„¹ì…˜ ì‹œì‘ =====
            # 9. XAI ë¶„ì„ ê²°ê³¼ ì„¹ì…˜ (report_layout ëª¨ë“ˆ ì‚¬ìš©)
            if xai_summary or xai_charts:
                print(f"ğŸ¨ Adding XAI section with {len(xai_charts)} visualizations and summary stats")
            else:
                print("â„¹ï¸  No XAI data available, skipping XAI section")
                
            if xai_summary or xai_charts:
                html_parts.append("""
            <div style="margin-bottom: 40px; padding: 25px; background: #fff3cd; border-radius: 12px; border: 2px solid #ffc107;">
                <h2 style="color: #495057; margin-bottom: 20px; padding-bottom: 12px; border-bottom: 3px solid #ffc107; font-size: 1.6em;">
                    ğŸ§  XAI (Explainable AI) Analysis Results
                </h2>
            """)
                    
                # XAI ë¶„ì„ ê²°ê³¼ ì„¹ì…˜ (report_layout ëª¨ë“ˆ ì‚¬ìš©)
                html_parts.append(generate_xai_analysis_section(xai_summary, xai_charts))
                    
                html_parts.append("""
            </div>
            """)
                
            # ===== XAI ë¶„ì„ ì„¹ì…˜ ì¢…ë£Œ =====
            
            return ''.join(html_parts)
                
        except ImportError as e:
            print(f"âŒ Error importing report_layout functions: {e}")
            return None
        except Exception as e:
            print(f"âŒ Error generating HTML: {e}")
            return None

def create_report_body(directory):
    """report_layout.pyì— ë§ëŠ” HTML ë³¸ë¬¸ë§Œ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        report = ImageAnalysisReport(directory)
        return report.generate_html_body()
    except Exception as e:
        # ì—ëŸ¬ê°€ ë°œìƒí•˜ë©´ Noneì„ ë°˜í™˜í•˜ì—¬ ë³´ê³ ì„œ ìƒì„±ì„ ì¤‘ë‹¨
        print(f"âŒ Error in create_report_body: {e}")
        return None

def create_xai_guideline_report(output_dir="."):
    """XAI ê°€ì´ë“œë¼ì¸ ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    if create_xai_guideline is None:
        print("âš ï¸  XAI guideline generator not available. Skipping guideline generation.")
        return None
    
    try:
        output_path = os.path.join(output_dir, "xai_guideline.html")
        create_xai_guideline(output_path)
        print(f"ğŸ“š XAI ê°€ì´ë“œë¼ì¸ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {output_path}")
        return output_path
    except Exception as e:
        print(f"âŒ Error creating XAI guideline: {e}")
        return None

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 2:
        print("Usage: python create_report.py <directory> [--guideline]")
        sys.exit(1)
    
    directory = sys.argv[1]
    
    # ê°€ì´ë“œë¼ì¸ ìƒì„± ì˜µì…˜ í™•ì¸
    generate_guideline = "--guideline" in sys.argv
    
    # ë©”ì¸ ë³´ê³ ì„œ ìƒì„±
    body_content = create_report_body(directory)
    print("Generated HTML body content:")
    print(body_content)
    
    # ê°€ì´ë“œë¼ì¸ ìƒì„± (ì˜µì…˜)
    if generate_guideline:
        create_xai_guideline_report()
    
    print(body_content)
