"""
MLflow-based Experiment Service for ddoc
Uses Ultralytics native MLflow integration
"""
import mlflow
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, List
from rich import print
from ultralytics import settings, YOLO

from .metadata_service import MetadataService
from ..ops.core_ops import CoreOpsPlugin


class MLflowExperimentService:
    """
    MLflow ê¸°ë°˜ ì‹¤í—˜ ì„œë¹„ìŠ¤ (Ultralytics ë„¤ì´í‹°ë¸Œ í†µí•©)
    - Git ì—†ì´ ìž‘ë™
    - ddoc ë°ì´í„° ë²„ì „ê³¼ ìžë™ ì—°ë™
    - ê³„ë³´ ê·¸ëž˜í”„ì— ì‹¤í—˜ ì¶”ê°€
    """
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.experiments_dir = self.project_root / "experiments"
        self.mlruns_dir = self.project_root / "mlruns"
        
        # MLflow tracking URI ì„¤ì •
        tracking_uri = f"file://{self.mlruns_dir.absolute()}"
        os.environ['MLFLOW_TRACKING_URI'] = tracking_uri
        mlflow.set_tracking_uri(tracking_uri)
        
        # Ultralytics MLflow í†µí•© í™œì„±í™”
        settings.update({
            "mlflow": True,
            "runs_dir": str(self.experiments_dir)  # ì‹¤í—˜ ê²°ê³¼ ì €ìž¥ ìœ„ì¹˜
        })
        
        # ddoc ì„œë¹„ìŠ¤ ì—°ë™
        self.metadata_service = MetadataService(project_root)
        self.core_ops = CoreOpsPlugin(project_root)
    
    def run_experiment(
        self,
        dataset_name: str,
        dataset_version: str,
        model: str = "yolov8n.pt",
        params: Dict[str, Any] = None,
        plugin: str = "yolo"
    ) -> Dict[str, Any]:
        """
        MLflowë¥¼ ì‚¬ìš©í•œ ì‹¤í—˜ ì‹¤í–‰ (Ultralytics ë„¤ì´í‹°ë¸Œ í†µí•©)
        
        Args:
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            dataset_version: ë°ì´í„°ì…‹ ë²„ì „
            model: ëª¨ë¸ ê²½ë¡œ
            params: í•™ìŠµ íŒŒë¼ë¯¸í„°
            plugin: í”ŒëŸ¬ê·¸ì¸ ì´ë¦„
        
        Returns:
            ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        params = params or {}
        exp_id = f"exp_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        dataset_id = f"{dataset_name}@{dataset_version}"
        
        try:
            # MLflow experiment ì„¤ì • (ddoc namespace)
            mlflow.set_experiment("ddoc")
            
            # MLflow run ì‹œìž‘ (context managerë¡œ ìžë™ ì¢…ë£Œ)
            with mlflow.start_run(run_name=exp_id) as run:
                
                # 1. ddoc ë©”íƒ€ë°ì´í„°ë¥¼ MLflow íƒœê·¸ë¡œ ì„¤ì •
                mlflow.set_tags({
                    "ddoc.dataset_name": dataset_name,
                    "ddoc.dataset_version": dataset_version,
                    "ddoc.dataset_id": dataset_id,
                    "ddoc.experiment_id": exp_id,
                    "ddoc.plugin": plugin
                })
                
                # 2. YOLO í•™ìŠµ ì‹¤í–‰
                # Ultralyticsê°€ ìžë™ìœ¼ë¡œ ëª¨ë“  ê²ƒì„ MLflowì— ë¡œê¹…
                print(f"[cyan]ðŸ”¬ Starting experiment: {exp_id}[/cyan]")
                print(f"[blue]ðŸ“Š MLflow Run ID: {run.info.run_id}[/blue]")
                
                yolo_model = YOLO(model)
                results = yolo_model.train(
                    data=params.get('data_yaml'),
                    epochs=params.get('epochs', 100),
                    batch=params.get('batch', 16),
                    imgsz=params.get('imgsz', 640),
                    device=params.get('device', 'cpu'),
                    project=str(self.experiments_dir),
                    name=exp_id,
                    exist_ok=True
                )
                
                # 3. í•™ìŠµ ê²°ê³¼ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                metrics = self._extract_metrics(results)
                
                # 4. ddoc ë©”íƒ€ë°ì´í„° ì €ìž¥
                self._save_ddoc_metadata(
                    exp_id=exp_id,
                    dataset_name=dataset_name,
                    dataset_version=dataset_version,
                    params=params,
                    metrics=metrics,
                    mlflow_run_id=run.info.run_id
                )
                
                # 5. ê³„ë³´ ê·¸ëž˜í”„ì— ì—°ê²°
                self._link_to_lineage(
                    exp_id=exp_id,
                    mlflow_run_id=run.info.run_id,
                    dataset_id=dataset_id,
                    params=params,
                    metrics=metrics,
                    plugin=plugin
                )
                
                print(f"[green]âœ… Experiment completed: {exp_id}[/green]")
                print(f"[blue]ðŸ”— Linked to dataset: {dataset_id}[/blue]")
                
                return {
                    "success": True,
                    "experiment_id": exp_id,
                    "mlflow_run_id": run.info.run_id,
                    "dataset_id": dataset_id,
                    "metrics": metrics,
                    "results_dir": str(self.experiments_dir / exp_id),
                    "timestamp": datetime.now().isoformat()
                }
                
        except Exception as e:
            print(f"[red]âŒ Experiment failed: {e}[/red]")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "experiment_id": exp_id,
                "error": str(e)
            }
    
    def _extract_metrics(self, results) -> Dict[str, Any]:
        """YOLO í•™ìŠµ ê²°ê³¼ì—ì„œ ë©”íŠ¸ë¦­ ì¶”ì¶œ"""
        try:
            # results.results_dictì—ì„œ ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ì¶œ
            metrics_dict = results.results_dict if hasattr(results, 'results_dict') else {}
            
            return {
                'mAP50': float(metrics_dict.get('metrics/mAP50(B)', 0)),
                'mAP50-95': float(metrics_dict.get('metrics/mAP50-95(B)', 0)),
                'precision': float(metrics_dict.get('metrics/precision(B)', 0)),
                'recall': float(metrics_dict.get('metrics/recall(B)', 0)),
                'fitness': float(metrics_dict.get('fitness', 0))
            }
        except Exception as e:
            print(f"[yellow]Warning: Could not extract metrics: {e}[/yellow]")
            return {}
    
    def _save_ddoc_metadata(
        self,
        exp_id: str,
        dataset_name: str,
        dataset_version: str,
        params: Dict[str, Any],
        metrics: Dict[str, Any],
        mlflow_run_id: str
    ):
        """ddoc ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ìž¥"""
        exp_dir = self.experiments_dir / exp_id
        
        metadata = {
            "experiment_id": exp_id,
            "mlflow_run_id": mlflow_run_id,
            "dataset": {
                "name": dataset_name,
                "version": dataset_version,
                "id": f"{dataset_name}@{dataset_version}"
            },
            "params": params,
            "metrics": metrics,
            "created_at": datetime.now().isoformat(),
            "mlflow_tracking_uri": os.environ.get('MLFLOW_TRACKING_URI'),
            "view_command": f"mlflow ui --backend-store-uri {self.mlruns_dir}"
        }
        
        metadata_file = exp_dir / "ddoc_metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def _link_to_lineage(
        self,
        exp_id: str,
        mlflow_run_id: str,
        dataset_id: str,
        params: Dict[str, Any],
        metrics: Dict[str, Any],
        plugin: str
    ):
        """ì‹¤í—˜ì„ ddoc ê³„ë³´ ê·¸ëž˜í”„ì— ì—°ê²°"""
        self.metadata_service.add_experiment(
            experiment_id=exp_id,
            experiment_name=exp_id,
            dataset_id=dataset_id,
            metadata={
                "mlflow_run_id": mlflow_run_id,
                "plugin": plugin,
                "params": params,
                "metrics": metrics,
                "timestamp": datetime.now().isoformat(),
                "tracking_type": "mlflow_ultralytics"
            }
        )
    
    def get_experiments_by_dataset(
        self,
        dataset_name: str,
        dataset_version: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """íŠ¹ì • ë°ì´í„°ì…‹ì˜ ëª¨ë“  ì‹¤í—˜ ì¡°íšŒ"""
        if dataset_version:
            filter_string = f"tags.`ddoc.dataset_id` = '{dataset_name}@{dataset_version}'"
        else:
            filter_string = f"tags.`ddoc.dataset_name` = '{dataset_name}'"
        
        try:
            runs = mlflow.search_runs(
                experiment_names=["ddoc"],
                filter_string=filter_string,
                order_by=["start_time DESC"]
            )
            return runs.to_dict('records') if not runs.empty else []
        except Exception as e:
            print(f"[yellow]Warning: Could not search MLflow runs: {e}[/yellow]")
            return []
    
    def compare_experiments(
        self,
        exp_ids: List[str]
    ) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì‹¤í—˜ ë¹„êµ"""
        comparison = {
            "experiments": [],
            "metrics_comparison": {}
        }
        
        for exp_id in exp_ids:
            try:
                runs = mlflow.search_runs(
                    experiment_names=["ddoc"],
                    filter_string=f"tags.`ddoc.experiment_id` = '{exp_id}'"
                )
                
                if not runs.empty:
                    run = runs.iloc[0].to_dict()
                    comparison["experiments"].append({
                        "experiment_id": exp_id,
                        "mlflow_run_id": run['run_id'],
                        "dataset_id": run.get('tags.ddoc.dataset_id'),
                        "metrics": {
                            k.replace('metrics.', ''): v 
                            for k, v in run.items() 
                            if k.startswith('metrics.') and v is not None
                        }
                    })
            except Exception as e:
                print(f"[yellow]Warning: Could not retrieve experiment {exp_id}: {e}[/yellow]")
        
        return comparison
    
    def get_best_experiment_for_dataset(
        self,
        dataset_name: str,
        dataset_version: str,
        metric: str = "metrics.mAP50-95"
    ) -> Optional[Dict[str, Any]]:
        """ë°ì´í„°ì…‹ ë²„ì „ì˜ ìµœê³  ì„±ëŠ¥ ì‹¤í—˜ ì°¾ê¸°"""
        runs = self.get_experiments_by_dataset(dataset_name, dataset_version)
        
        if not runs:
            return None
        
        # ë©”íŠ¸ë¦­ ê¸°ì¤€ ì •ë ¬
        valid_runs = [r for r in runs if r.get(metric) is not None]
        if not valid_runs:
            return None
        
        best_run = max(valid_runs, key=lambda x: float(x.get(metric, 0)))
        
        return best_run


# ì‹±ê¸€í†¤
_mlflow_exp_service = None


def get_mlflow_experiment_service(project_root: str = ".") -> MLflowExperimentService:
    """Get global MLflow experiment service instance"""
    global _mlflow_exp_service
    if _mlflow_exp_service is None:
        _mlflow_exp_service = MLflowExperimentService(project_root)
    return _mlflow_exp_service

